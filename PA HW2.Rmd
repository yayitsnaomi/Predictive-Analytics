---
title: "Naomi Kaduwela - Predictive Analytics HW2"
output: html_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)

rm(list=ls())
library(readxl)
library(nnet)
library(ALEPlot)
library(rpart)

```


```{r}
#1a.) Fit a linear regression model to the ischemic heart disease data. 

heart  <- read_excel("HW2_data.xls")
heart$cost <- log10(heart$cost) #take log of cost
heart$gend <-as.factor(heart$gend) #factor for gender
heart <- heart[,c(-1)]

# Linear Regression 
lm <- lm(cost ~ age + gend + intvn + drugs + ervis + comp + comorb + dur, data = heart)
summary(lm)
```


1a.) Using any and all arguments that are relevant, discuss how well the model fits the data in terms of its predictive power.

Multiple R-squared:  0.5831, Adjusted R-squared: 0.5789. F-statistic: 136.2

1b.) Which variables appear to have the most influence on the cost?

Age, gender, and the # of drugs prescribed do not have significant p values.
Emergency room visits is slightly significant. 
All the remaining predictors are highly significant - number of intervention nprocedures,
complaints, comorbidities, and the duration of the treament condition. 


```{r}
#c.)#Linear Regression Residuals - diagnostic plots 
layout(matrix(c(1,2,3,4),2,2)) # optional 4 graphs/page 
```

```{r}
#plot all predictor variables
plot(heart)
```





1c.)  Construct appropriate diagnostics and residual plots to assess whether you think there are any problems with the data set that require remedial action or any nonlinearity in the relationship between the response and the predictors.
Looking at the residuals they clearly do not seem randomlyl dispursed, They are bunched on the left side and then arc. They seem to follow a log like relationship between x and y.
The QQ plot also shows deviation from the line, showing heavy tails in the data.
From the residuals vs leverage plot there also seem to be some outliers. 
The scatterplot matrix also shows nonlinear relationships between log cost and predictors like intervention and comorbidities have heavy non linear relationships.

```{r}
#2 - Neural Network 

#read in data and take log of cost
heart  <- read_excel("HW2_data.xls")
heart$cost <- log10(heart$cost) #take log of cost
heart$gend <-as.factor(heart$gend) #factor for gender
heart <- heart[,c(-1)]

#leveraging the data from previous question, including log cost, this will be standardized and scaled version of data for NN
#(except for gender because it's a factor) 
heart[1:2]<-sapply(heart[1:2], function(x) (x-mean(x))/sd(x)) #standardize predictors
heart[4:9]<-sapply(heart[4:9], function(x) (x-mean(x))/sd(x)) #standardize predictors
#View(heart_scaled)

```



```{r}
#######A function to determine the indices in a CV partition##################
CVInd <- function(n,K) {  #n is sample size; K is number of parts; returns K-length list of indices for each part
  m<-floor(n/K)  #approximate size of each part
  r<-n-m*K  
  I<-sample(n,n)  #random reordering of the indices
  Ind<-list()  #will be list of indices for all K parts
  length(Ind)<-K
  for (k in 1:K) {
    if (k <= r) kpart <- ((m+1)*(k-1)+1):((m+1)*k)  
    else kpart<-((m+1)*r+m*(k-r-1)+1):((m+1)*r+m*(k-r))
    Ind[[k]] <- I[kpart]  #indices for kth part of data
  }
  Ind
}
```

```{r}
#2a.) Use 10-fold cross-validation to find the best combination of shrinkage parameter λ and number of hidden nodes.
set.seed(12345)

##Now use multiple reps of CV to compare Neural Nets and linear reg models###
Nrep<-3 #number of replicates of CV
K<-10  #K-fold CV on each replicate
n.models = 9 #number of different models to fit
n=nrow(heart)
y<-heart$cost
yhat=matrix(0,n,n.models)
MSE<-matrix(0,Nrep,n.models)
size_values = list(1,11, 20);
decay_values = list(.01, 1, .3);
for (j in 1:Nrep) {
  Ind<-CVInd(n,K)
  q = 1
  for (s in size_values){ #loop for size

    for (d in decay_values){#loop for decay 
      for (k in 1:K) {
        out<-nnet(cost~.,heart[-Ind[[k]],], linout=T, skip=F, size=s, decay=d, maxit=1000, trace=F)
        yhat[Ind[[k]],q]<-as.numeric(predict(out,heart[Ind[[k]],]))
        
      } #end of k loop
      q=q+1
      MSE[j,]=apply(yhat,2,function(x) sum((y-x)^2))/n
    } #end of j loop
  }
}
#MSE
MSEAve<- apply(MSE,2,mean); 
paste("MSEAvg",MSEAve) #averaged mean square CV error

MSEsd <- apply(MSE,2,sd); 
MSEsd   #SD of mean square CV error

r2<- 1-MSEAve/var(y); 
paste("R^2",r2)  #CV r^2

```

2a.) Use 10-fold cross-validation to find the best combination of shrinkage parameter λ and number of hidden nodes.
The highest R^2 from CV is: R^2 0.681779904785314
with parameters size =11 and decay = 2

```{r}
#2b.) Fit the final best model and discuss how good you think the model is, in terms of its predictive power.

set.seed(12345)

#CV with the final NN parameters
Ind<-CVInd(n=nrow(heart),10)
K<-length(Ind)
y<-heart$cost
yhat<-y
for (k in 1:K) {
  out<-nnet(cost~.,heart[- Ind[[k]],],linout=T,skip=T,size=11,decay=2,maxit=1000,trace=F)
  yhat[Ind[[k]]]<-as.numeric(predict(out,heart[Ind[[k]],])) }

e1=y-yhat
nn_r2 =1-var(e1)/var(y)
nn_r2 #0.6756784

#summary(nn1)

#now compare to linear regression with same CV index partition 
for (k in 1:K) {
  out <- lm(cost ~ ., heart[-Ind[[k]], ])
  yhat[Ind[[k]]] <- as.numeric(predict(out, heart[Ind[[k]], ]))
}
e2=y-yhat 
lm_r2 = 1-var(e2)/var(y)
lm_r2 #0.5680435


```
2b.) Fit the final best model and discuss how good you think the model is, in terms of its predictive power.
The neural net did perform better than the neural net. 
The Neural net r^2= 0.6756784, whereas the linear model r^2 is 0.5680435


```{r}
#2c.) ALE plots

heart <- cbind(heart[,2:9],heart[,c(1)])
colnames(heart) <- c("age","gend","intvn","drugs","ervis","comp","comorb","dur","cost")
nn1 <- nnet(cost~.,heart[- Ind[[k]],],linout=T,skip=T,size=20,decay=1,maxit=1000,trace=F)

#ALEPlot
heart <- data.frame(heart)
yhat <- function(X.model, newdata) as.numeric(predict(X.model, newdata)) 
par(mfrow=c(2,4))
for (j in 1:8) {
  ALEPlot(data.frame(heart[,1:8]), nn1, pred.fun=yhat, J=j, K=50, NA.plot = TRUE) 
  rug(heart[,j]) 
  } ## This creates main effect ALE plots for all 8 predictors

par(mfrow=c(1,1))

```

2c.) Which variables appear to have the most influence on the cost, and what are their effects? You can use the ALEPlot package for this.
Looking at the ALE plots, it seems that number of intervensions (log like relationship), emergency room visits (linear relationship), and comorbidities (log like relationship) are the most significant in relation to cost because they have the largest y axis scale. 

```{r}
#2d.) Residual plots
plot(nn1$fitted.values, nn1$residuals)

```

2d.) Construct appropriate residual plots to assess whether there remains any nonlinearity not captured by the neural network model.
The neural net residuals look more random now. It has captured some of the nonlinear relationships that the linear model did not.


```{r}
#3. Regression Tree 

set.seed(12345)
heart  <- read_excel("HW2_data.xls")
heart$cost <- log10(heart$cost)
heart$gend <-as.factor(heart$gend) #factor for gender

heart <- heart[,c(-1)]

#3a.) Use 10-fold cross-validation to find the best tree size or complexity parameter value.

control <- rpart.control(minbucket = 5, cp = 0.0001, maxsurrogate = 0, usesurrogate = 0, xval = 10) #10 fold CV because xval = 10
INC.tr <- rpart(cost ~ ., heart, method = "anova", control = control) 
plotcp(INC.tr) #optimal CP = 0.002185197 @ 21 clusters
printcp(INC.tr) 

#prune back to optimal size, according to plot of CV r^2
min_rel_error_position <- which.min(INC.tr$cptable[,4]) # find position of CP for minimum x error = 21
cp_value <- INC.tr$cptable[min_rel_error_position,1] # find cp corresponding of CP to minimum x error = 0.002185197

INC.tr1 <- prune(INC.tr, cp=cp_value) #approximately the cp corresponding to the best size 
#INC.tr1

```
3a.) Use 10-fold cross-validation to find the best tree size or complexity parameter value.
Looking at the x-val relative error and CP chart - we see a dip at CP = 0.002185197 associated with 21 clusters, which we will take as optimal CP.

```{r}
{par(cex=.9) 
plot(INC.tr1, uniform=F)
text(INC.tr1, use.n = F)
par(cex=1)}

INC.tr1$cptable[nrow(INC.tr1$cptable),] #shows training and CV r^2
#rel error = training misclass rate = 0.262736248
#xerror = CV miscalss rate = 0.356927546
#Rt^2 = 1-relative error = 0.7372638
#Rcv^2 = 1-x error = 0.6430725

  
yhat<-predict(INC.tr1); 
e<-heart$cost-yhat
c(1-var(e)/var(heart$cost), 1-INC.tr1$cptable[nrow(INC.tr1$cptable),3]) #check to see training r^2 agrees with what is in cptable
```
3b.) Fit the final best model and discuss how good you think the model is, in terms of its predictive power.
The regression tree is better than the linear model (R^2 = 0.5831).
The regression tree is slightly worse than the neural net (r^2= 0.6756784).

rel error = training misclass rate = 0.262736248
xerror = CV miscalss rate = 0.356927546
Rt^2 = 1-relative error = 0.7372638
Rcv^2 = 1-x error = 0.6430725

```{r}
INC.tr1$variable.importance #intvn > duration > comorb
```

3c.) Which variables appear to have the most influence on the cost, and what are their effects?
Number of interventions seems to be the most important variable. Higher number of interventions result in higher total cost. 
Duration and comorbidities seem fairly important too, that that respective order.

```{r}
#3
#(d) fitted values vs residuals
plot(yhat,e) #fitted values vs residuals

```

3d.) Construct appropriate residual plots to assess whether there remains any linearity not captured by the regression tree model.
The residual vs fitted values plot for regression trees seems to look random.
The vertical bunching up of variables is expected due to the nature of trees: because there are binary splits and the final leaf nodes result in all values falling into that bucket having the same predicted Y value.

3e.) Which model (the linear regression, neural network, or tree) would you recommend for this data set, and why?
The neural net has the highest cross valdiation R^2, thus that is the strongest model in terms of predictive power.
However, the regression tree is close in predictive power to the neural net (0.6430725 vs the neural net's R^2 of 0.6756784),
so if explainability is important, it is worth the predictive power trade off of .03 to use the regression tree.
Definitely the linear model should not be used because the R^2 is only 0.5831 and there are clear non linear relationships between the predictors and cost that it misses.

```{r}
#4
library(MASS)

#(a) Use 10-fold cross-validation to find the best neural network model for classifying the class type.

FGL<-read.table("fgl.txt",sep="\t")
#factor Y predictor so it can interpert it as categorical for classification 
FGL$type <- as.factor(FGL$type)

#standardize predictors 
FGL1 <- FGL
FGL1[1:9]<-sapply(FGL1[1:9], function(x) (x-mean(x))/sd(x)) #standardize predictors by subtracting mean and dividing by SD
pairs(FGL1, cex=.5, pch=16) 

##Now use multiple reps of CV to compare Neural Nets and linear reg models###
set.seed(12345)
Nrep<-3 #number of replicates of CV
K<-10  #K-fold CV on each replicate
n.models = 6 #number of different models to fit
n=nrow(FGL1)
y<-FGL1$type
yhat=matrix(0,n,n.models)
CVrate<-matrix(0,Nrep,n.models)
size_values = list(23,25)
decay_values = list(0.01,0.05,0.1)

for (j in 1:Nrep) {
  Ind<-CVInd(n,K)
  q = 1
  for (s in size_values){ #loop for size
    for (d in decay_values){#loop for decay 
      for (k in 1:K) {
        out<- nnet(type~.,FGL1[-Ind[[k]],], linout=F, skip=F, size=s, decay=d, maxit=1000, trace=F)
        yhat[Ind[[k]],q]<-predict(out,FGL1[Ind[[k]],],type="class")
 
      } #end of k loop
      q=q+1
      CVrate[j,]=apply(yhat,2,function(x) sum(y != x)/length(y)) #misclass rate
    } #end of j loop
  }
}



#misclass rate
CVrateAvg<- apply(CVrate,2,mean); 
min_cv_position <- which.min(CVrateAvg) # min misclass rate position = 2
min(CVrateAvg) #0.2772586
paste("min CVrateAvg",CVrateAvg) #averaged min misclass rate


```



```{r}

#refit final neural network model with CV

set.seed(12345)

#CV with the final NN parameters
Nrep<-3 #number of replicates of CV
Ind<-CVInd(n=nrow(FGL1),10)
K<-length(Ind)
y<-FGL1$type
yhat<-y
  for (k in 1:K) {
    out<- nnet(type~.,FGL1[-Ind[[k]],], linout=F, skip=F, size=25, decay=0.05, maxit=100, trace=F)
    yhat[Ind[[k]]]<-predict(out,FGL1[Ind[[k]],],type="class")
  }
CVrate =sum(y != yhat)/length(y) #misclass rate =0.2850467
CVrate

```

4a.) Use 10-fold cross-validation to find the best neural network model for classifying the class type
The minimum misclass rate = 0.2850467, when we use size = 25 and decay = 0.05 as parameters for the neural network.

```{r}
#(b) Classification tree

set.seed(12345)

FGL<-read.table("fgl.txt",sep="\t",header=TRUE,strip.white=TRUE,na.strings="?")
control <- rpart.control(minbucket = 5, cp = 0.00001, maxsurrogate = 0, usesurrogate = 0, xval = 10)
FGL.tr <- rpart(type ~ ., FGL, method = "class", control = control) 
plotcp(FGL.tr)
printcp(FGL.tr)

```

```{r}
#prune back to optimal size, according to plot of CV r^2
min_rel_error_position <- which.min(FGL.tr$cptable[,4]) # find position of CP for minimum x error = 7
cp_value <- FGL.tr$cptable[min_rel_error_position,1] # find cp corresponding of CP to minimum x error = 0.007246377


FGL.tr1 <- prune(FGL.tr, cp=0.007246377) #approximately the cp corresponding to the best size
#FGL.tr1
{par(cex=.7); 
plot(FGL.tr1, uniform=F); 
text(FGL.tr1, use.n = F); }
par(cex=1) 


yhat<-predict(FGL.tr1, type="class"); 
sum(yhat != FGL$type)/nrow(FGL) #check training misclass rate =  0.2196262
```

4b.) Use 10-fold cross-validation to find the best classification tree model for classifying the class type.
The best classification model has misclas rate = 0.2196262, with CP = 0.007246377.
The regression tree was better than the neural net (which had misclass rate = 0.2850467)

```{r}
# (c) Multinomial 
set.seed(12345)
fit<-multinom(type ~., FGL)
summary(fit)

yhat<-predict(fit, type="class"); 
sum(yhat != FGL$type)/nrow(FGL) #check training misclass rate =  0.2663551
```
4c.) An alternative to a neural network or classification tree is to use nominal logistic regression, which is like the binary logistic regression with which you are familiar. Fit a multinomial model and discuss the results.
The multinomial has a misclass rate = 0.2663551, which is better than the neural net, but wose than the regression tree.

4d.) Compare the three models from parts (a)—(c) in terms of their predictive ability and interpretability. Which model do you think is the most appropriate for predicting glass type?
The classification tree has the best (lowest) misclassification rate (=0.2196262) and it is interpertable, thus I would choose the classification tree.
The multinomial misclass rate (=0.2663551) and neural net misclassification rate (=0.2850467) are both .05-.06 higher, respectively. 


